{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231b9e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1f21ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -q xarray netCDF4 matplotlib seaborn scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cfbab6",
   "metadata": {},
   "source": [
    "# GCM Downscaling Data Inspection Notebook\n",
    "\n",
    "**Environment**: Google Colab  \n",
    "**Purpose**: Inspect NetCDF climate data files (CRU, ERA5, GCMs) to verify variable names, coordinates, temporal coverage, and spatial grids.\n",
    "\n",
    "## Setup Instructions:\n",
    "1. Run the installation cell to install required packages\n",
    "2. Mount Google Drive and verify file paths\n",
    "3. Execute inspection cells sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143caa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up paths for Google Drive\n",
    "base_path = Path('/content/drive/MyDrive/Downscaling ML CEP/AI_GCMs')\n",
    "cru_path = base_path / 'CRU'\n",
    "era5_path = base_path / 'ERA5'\n",
    "gcm_path = base_path / 'GCMs'\n",
    "\n",
    "# Verify paths exist\n",
    "print(\"Verifying data paths...\")\n",
    "for path_name, path in [('Base', base_path), ('CRU', cru_path), ('ERA5', era5_path), ('GCM', gcm_path)]:\n",
    "    if path.exists():\n",
    "        print(f\"✓ {path_name} path exists: {path}\")\n",
    "    else:\n",
    "        print(f\"✗ {path_name} path NOT FOUND: {path}\")\n",
    "        print(f\"  Please create this directory in Google Drive\")\n",
    "\n",
    "# Create outputs directory\n",
    "output_path = Path('/content/drive/MyDrive/Downscaling ML CEP/outputs/figures')\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"\\n✓ Output directory ready: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb6d861",
   "metadata": {},
   "source": [
    "## 1. Inspect CRU Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecb3e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_netcdf(filepath, show_sample=True):\n",
    "    \"\"\"Comprehensive NetCDF file inspection\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FILE: {filepath.name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        ds = xr.open_dataset(filepath)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: File not found at {filepath}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not open file - {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Dimensions\n",
    "    print(f\"\\nDIMENSIONS:\")\n",
    "    for dim, size in ds.sizes.items():\n",
    "        print(f\"  {dim}: {size}\")\n",
    "    \n",
    "    # Data variables\n",
    "    print(f\"\\nDATA VARIABLES:\")\n",
    "    for var in ds.data_vars:\n",
    "        var_obj = ds[var]\n",
    "        print(f\"  {var}:\")\n",
    "        print(f\"    Shape: {var_obj.shape}\")\n",
    "        print(f\"    Dtype: {var_obj.dtype}\")\n",
    "        print(f\"    Dims: {var_obj.dims}\")\n",
    "        \n",
    "        # Attributes\n",
    "        for attr in ['units', 'long_name', 'standard_name']:\n",
    "            if hasattr(var_obj, attr):\n",
    "                print(f\"    {attr}: {getattr(var_obj, attr)}\")\n",
    "        \n",
    "        # Statistics (only for numeric data)\n",
    "        if np.issubdtype(var_obj.dtype, np.number):\n",
    "            try:\n",
    "                print(f\"    Min: {float(var_obj.min().values):.4f}\")\n",
    "                print(f\"    Max: {float(var_obj.max().values):.4f}\")\n",
    "                print(f\"    Mean: {float(var_obj.mean().values):.4f}\")\n",
    "                print(f\"    NaN count: {np.isnan(var_obj.values).sum()}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Statistics: Unable to compute ({e})\")\n",
    "        else:\n",
    "            print(f\"    Data type: Non-numeric ({var_obj.dtype})\")\n",
    "            print(f\"    Sample values: {var_obj.values.flat[:3]}\")\n",
    "    \n",
    "    # Coordinates\n",
    "    print(f\"\\nCOORDINATES:\")\n",
    "    for coord in ds.coords:\n",
    "        coord_obj = ds[coord]\n",
    "        print(f\"  {coord}:\")\n",
    "        print(f\"    Shape: {coord_obj.shape}\")\n",
    "        print(f\"    Dtype: {coord_obj.dtype}\")\n",
    "        \n",
    "        if coord_obj.size > 0:\n",
    "            if 'time' in coord.lower():\n",
    "                try:\n",
    "                    print(f\"    Range: {pd.to_datetime(coord_obj.values[0])} to {pd.to_datetime(coord_obj.values[-1])}\")\n",
    "                    print(f\"    First 3: {[pd.to_datetime(t) for t in coord_obj.values[:3]]}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    Values: {coord_obj.values[:3]} (time parsing failed)\")\n",
    "            elif coord_obj.size <= 10:\n",
    "                print(f\"    Values: {coord_obj.values}\")\n",
    "            else:\n",
    "                if np.issubdtype(coord_obj.dtype, np.number):\n",
    "                    print(f\"    Range: {coord_obj.values.min():.4f} to {coord_obj.values.max():.4f}\")\n",
    "                    print(f\"    First 3: {coord_obj.values[:3]}\")\n",
    "                    print(f\"    Last 3: {coord_obj.values[-3:]}\")\n",
    "                else:\n",
    "                    print(f\"    First 3: {coord_obj.values[:3]}\")\n",
    "                    print(f\"    Last 3: {coord_obj.values[-3:]}\")\n",
    "        \n",
    "        # Units\n",
    "        if hasattr(coord_obj, 'units'):\n",
    "            print(f\"    Units: {coord_obj.units}\")\n",
    "    \n",
    "    # Global attributes\n",
    "    print(f\"\\nGLOBAL ATTRIBUTES:\")\n",
    "    for attr, value in ds.attrs.items():\n",
    "        print(f\"  {attr}: {str(value)[:100]}\")\n",
    "    \n",
    "    if show_sample:\n",
    "        print(f\"\\nSAMPLE DATA (first time slice):\")\n",
    "        print(ds.isel(time=0) if 'time' in ds.dims else ds.isel({list(ds.dims.keys())[0]: 0}))\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330e6cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect CRU temperature\n",
    "cru_tmp = inspect_netcdf(cru_path / 'cru_tmp.1901.2024.0.25deg.pakistan.nc', show_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4710e90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect CRU precipitation\n",
    "cru_pre = inspect_netcdf(cru_path / 'cru_pre.1901.2024.0.25deg.pakistan.nc', show_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0752e318",
   "metadata": {},
   "source": [
    "## 2. Inspect ERA5 Target Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7103937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect ERA5 file 1 (avgua - likely upward/temperature)\n",
    "era5_ua = inspect_netcdf(era5_path / 'data_stream-moda_stepType-avgua.nc', show_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a529375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect ERA5 file 2 (avgad - likely downward/precipitation)\n",
    "era5_ad = inspect_netcdf(era5_path / 'data_stream-moda_stepType-avgad.nc', show_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f1af35",
   "metadata": {},
   "source": [
    "## 3. Inspect GCM Historical and Future Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccf40fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect sample GCM files (BCC-CSM2-MR)\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# GCM HISTORICAL DATA\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "gcm_hist_tas = inspect_netcdf(gcm_path / 'BCC-CSM2-MR_hist_tas.nc', show_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be18d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcm_hist_pr = inspect_netcdf(gcm_path / 'BCC-CSM2-MR_hist_pr.nc', show_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae9e01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# GCM FUTURE SCENARIOS\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "gcm_ssp126_tas = inspect_netcdf(gcm_path / 'BCC-CSM2-MR_ssp126_tas.nc', show_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6c16d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcm_ssp585_pr = inspect_netcdf(gcm_path / 'BCC-CSM2-MR_ssp585_pr.nc', show_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047457b7",
   "metadata": {},
   "source": [
    "## 4. Check another GCM for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4541af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CanESM5 for consistency\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# VERIFICATION: CanESM5\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "canesm_hist = inspect_netcdf(gcm_path / 'CanESM5_hist_tas.nc', show_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53ec9f2",
   "metadata": {},
   "source": [
    "## 5. Temporal Overlap Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf0fb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze temporal overlap for 1980-2014 training period\n",
    "def get_time_range(filepath):\n",
    "    \"\"\"Extract time range from NetCDF file\"\"\"\n",
    "    try:\n",
    "        ds = xr.open_dataset(filepath)\n",
    "        time_coord = None\n",
    "        \n",
    "        # Find time coordinate\n",
    "        for coord in ds.coords:\n",
    "            if 'time' in coord.lower():\n",
    "                time_coord = coord\n",
    "                break\n",
    "        \n",
    "        if time_coord:\n",
    "            time_vals = pd.to_datetime(ds[time_coord].values)\n",
    "            ds.close()\n",
    "            return time_vals[0], time_vals[-1], len(time_vals)\n",
    "        else:\n",
    "            ds.close()\n",
    "            return None, None, 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filepath.name}: {e}\")\n",
    "        return None, None, 0\n",
    "\n",
    "# Check temporal ranges\n",
    "datasets = {\n",
    "    'CRU Temperature': cru_path / 'cru_tmp.1901.2024.0.25deg.pakistan.nc',\n",
    "    'CRU Precipitation': cru_path / 'cru_pre.1901.2024.0.25deg.pakistan.nc',\n",
    "    'ERA5 avgua': era5_path / 'data_stream-moda_stepType-avgua.nc',\n",
    "    'ERA5 avgad': era5_path / 'data_stream-moda_stepType-avgad.nc',\n",
    "    'BCC-CSM2-MR hist tas': gcm_path / 'BCC-CSM2-MR_hist_tas.nc',\n",
    "    'BCC-CSM2-MR hist pr': gcm_path / 'BCC-CSM2-MR_hist_pr.nc',\n",
    "    'BCC-CSM2-MR ssp126 tas': gcm_path / 'BCC-CSM2-MR_ssp126_tas.nc',\n",
    "    'BCC-CSM2-MR ssp585 pr': gcm_path / 'BCC-CSM2-MR_ssp585_pr.nc',\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEMPORAL COVERAGE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Dataset':<30} {'Start':<12} {'End':<12} {'N timesteps':>12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for name, path in datasets.items():\n",
    "    start, end, n = get_time_range(path)\n",
    "    if start:\n",
    "        print(f\"{name:<30} {str(start)[:10]:<12} {str(end)[:10]:<12} {n:>12}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TARGET TRAINING PERIOD: 1980-01-01 to 2014-12-31\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aed0a21",
   "metadata": {},
   "source": [
    "## 6. Spatial Grid Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ff4ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare spatial grids\n",
    "def get_grid_info(filepath):\n",
    "    \"\"\"Extract spatial grid information\"\"\"\n",
    "    try:\n",
    "        ds = xr.open_dataset(filepath)\n",
    "        \n",
    "        lat_coord = None\n",
    "        lon_coord = None\n",
    "        \n",
    "        # Find lat/lon coordinates\n",
    "        for coord in ds.coords:\n",
    "            if 'lat' in coord.lower():\n",
    "                lat_coord = coord\n",
    "            if 'lon' in coord.lower():\n",
    "                lon_coord = coord\n",
    "        \n",
    "        if lat_coord and lon_coord:\n",
    "            lat = ds[lat_coord].values\n",
    "            lon = ds[lon_coord].values\n",
    "            \n",
    "            lat_res = np.diff(lat).mean() if len(lat) > 1 else 0\n",
    "            lon_res = np.diff(lon).mean() if len(lon) > 1 else 0\n",
    "            \n",
    "            ds.close()\n",
    "            return {\n",
    "                'lat_min': lat.min(),\n",
    "                'lat_max': lat.max(),\n",
    "                'lon_min': lon.min(),\n",
    "                'lon_max': lon.max(),\n",
    "                'n_lat': len(lat),\n",
    "                'n_lon': len(lon),\n",
    "                'lat_res': lat_res,\n",
    "                'lon_res': lon_res,\n",
    "            }\n",
    "        else:\n",
    "            ds.close()\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filepath.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SPATIAL GRID COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Dataset':<30} {'Lat Range':<20} {'Lon Range':<20} {'Grid':>15}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for name, path in datasets.items():\n",
    "    if 'ssp' not in name.lower():  # Skip future scenarios for now\n",
    "        grid = get_grid_info(path)\n",
    "        if grid:\n",
    "            lat_range = f\"{grid['lat_min']:.2f} to {grid['lat_max']:.2f}\"\n",
    "            lon_range = f\"{grid['lon_min']:.2f} to {grid['lon_max']:.2f}\"\n",
    "            grid_str = f\"{grid['n_lat']} x {grid['n_lon']} ({abs(grid['lat_res']):.3f}°)\"\n",
    "            print(f\"{name:<30} {lat_range:<20} {lon_range:<20} {grid_str:>15}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TARGET GRID: CRU 0.25° Pakistan (lat 23-38°N, lon 60-78°E)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740bbde5",
   "metadata": {},
   "source": [
    "## 7. Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5821883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick visualization of spatial patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Sample Spatial Patterns (First Time Step)', fontsize=14, fontweight='bold')\n",
    "\n",
    "try:\n",
    "    # CRU Temperature\n",
    "    ds_cru_tmp = xr.open_dataset(cru_path / 'cru_tmp.1901.2024.0.25deg.pakistan.nc')\n",
    "    var_name_tmp = list(ds_cru_tmp.data_vars)[0]\n",
    "    ds_cru_tmp[var_name_tmp].isel(time=0).plot(ax=axes[0, 0], cmap='RdYlBu_r')\n",
    "    axes[0, 0].set_title('CRU Temperature (first timestep)')\n",
    "\n",
    "    # CRU Precipitation\n",
    "    ds_cru_pre = xr.open_dataset(cru_path / 'cru_pre.1901.2024.0.25deg.pakistan.nc')\n",
    "    var_name_pre = list(ds_cru_pre.data_vars)[0]\n",
    "    ds_cru_pre[var_name_pre].isel(time=0).plot(ax=axes[0, 1], cmap='YlGnBu')\n",
    "    axes[0, 1].set_title('CRU Precipitation (first timestep)')\n",
    "\n",
    "    # GCM Temperature\n",
    "    ds_gcm_tas = xr.open_dataset(gcm_path / 'BCC-CSM2-MR_hist_tas.nc')\n",
    "    var_name_tas = list(ds_gcm_tas.data_vars)[0]\n",
    "    ds_gcm_tas[var_name_tas].isel(time=0).plot(ax=axes[1, 0], cmap='RdYlBu_r')\n",
    "    axes[1, 0].set_title('GCM Temperature (BCC-CSM2-MR, first timestep)')\n",
    "\n",
    "    # GCM Precipitation\n",
    "    ds_gcm_pr = xr.open_dataset(gcm_path / 'BCC-CSM2-MR_hist_pr.nc')\n",
    "    var_name_pr = list(ds_gcm_pr.data_vars)[0]\n",
    "    ds_gcm_pr[var_name_pr].isel(time=0).plot(ax=axes[1, 1], cmap='YlGnBu')\n",
    "    axes[1, 1].set_title('GCM Precipitation (BCC-CSM2-MR, first timestep)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save to Google Drive\n",
    "    save_path = output_path / '00_initial_spatial_patterns.png'\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\n✓ Figure saved to: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Clean up\n",
    "    ds_cru_tmp.close()\n",
    "    ds_cru_pre.close()\n",
    "    ds_gcm_tas.close()\n",
    "    ds_gcm_pr.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating visualization: {e}\")\n",
    "    print(\"Please verify all NetCDF files exist in the specified paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55ae607",
   "metadata": {},
   "source": [
    "## 8. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8a1096",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INSPECTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "Key Findings from Data Inspection:\n",
    "\n",
    "1. CRU Variables:\n",
    "   - Temperature: 'tmp' (degrees Celsius)\n",
    "   - Precipitation: 'pre' (mm/month)\n",
    "   - Coordinates: time, lat, lon\n",
    "   - Resolution: 0.25° (60 x 72 grid)\n",
    "   - Coverage: 1901-2024 (1488 timesteps)\n",
    "\n",
    "2. ERA5 Variables:\n",
    "   - Check avgua file for temperature variable\n",
    "   - Check avgad file for precipitation variable\n",
    "   - May need unit conversion (K→°C, m→mm)\n",
    "\n",
    "3. GCM Variables:\n",
    "   - Temperature: 'tas' (likely Kelvin)\n",
    "   - Precipitation: 'pr' (likely kg m⁻²s⁻¹)\n",
    "   - Resolution: Coarser than CRU (needs regridding)\n",
    "\n",
    "4. Coordinate Conventions:\n",
    "   - Use ds.sizes instead of ds.dims.items() (deprecated)\n",
    "   - Standard naming: time, lat, lon\n",
    "\n",
    "5. Required Preprocessing:\n",
    "   - Subset to 1980-2014 training period\n",
    "   - Regrid GCMs to CRU 0.25° grid\n",
    "   - Convert units (K→°C, kg m⁻²s⁻¹→mm/month)\n",
    "   - Align temporal indices across datasets\n",
    "\n",
    "Next Steps:\n",
    "1. Upload this notebook's output to verify ERA5 variable names\n",
    "2. Build preprocessing pipeline (src/data/preprocessors.py)\n",
    "3. Implement regridding using xESMF or xarray.interp\n",
    "4. Create feature engineering pipeline\n",
    "5. Train ML models\n",
    "\"\"\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GOOGLE COLAB PATHS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Base path: {base_path}\")\n",
    "print(f\"Output path: {output_path}\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
